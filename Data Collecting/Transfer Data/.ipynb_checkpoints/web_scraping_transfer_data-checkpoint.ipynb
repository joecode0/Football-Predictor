{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create urls we need\n",
    "base_url = \"https://www.transfermarkt.co.uk/premier-league/einnahmenausgaben/wettbewerb/GB1/plus/0?ids=a&sa=&\" \n",
    "url_end = \"&nat=&pos=&altersklasse=&w_s=&leihe=&intern=0/\"\n",
    "\n",
    "# tags of form \"saison_id=*start_year*&saison_id_bis=*start_year*\"\n",
    "url_dict = dict()\n",
    "for k in range(1995,2019):\n",
    "    tag = \"saison_id=\" + str(k) + \"&saison_id_bis=\" + str(k)\n",
    "    url_dict[k] = base_url + tag + url_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the page html\n",
    "response = requests.get(url_dict[2017], headers={'User-Agent': 'Custom'})\n",
    "page = soup(response.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all table entries\n",
    "odd_entries = page.findAll(\"tr\",{\"class\":\"odd\"})\n",
    "even_entries = page.findAll(\"tr\",{\"class\":\"even\"})\n",
    "entries = odd_entries + even_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'£285.75m'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write code to get each of the data points we want\n",
    "test_entry = odd_entries[0]\n",
    "# get expenditure\n",
    "expenditure_block = test_entry.findAll(\"td\",{\"class\":\"rechts hauptlink redtext\"})\n",
    "expenditure = expenditure_block[0].text\n",
    "expenditure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'£86.72m'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get income\n",
    "income_block = test_entry.findAll(\"td\",{\"class\":\"rechts hauptlink greentext\"})\n",
    "income = income_block[0].text\n",
    "income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Manchester City'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get team name\n",
    "team_block = test_entry.findAll(\"td\",{\"class\":\"hauptlink no-border-links\"})\n",
    "team_name = team_block[0].a.text\n",
    "team_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put this all together into one function\n",
    "def get_tuple_data(entry):\n",
    "    team_block = entry.findAll(\"td\",{\"class\":\"hauptlink no-border-links\"})\n",
    "    team_name = team_block[0].a.text\n",
    "    expenditure_block = entry.findAll(\"td\",{\"class\":\"rechts hauptlink redtext\"})\n",
    "    expenditure = expenditure_block[0].text\n",
    "    income_block = entry.findAll(\"td\",{\"class\":\"rechts hauptlink greentext\"})\n",
    "    income = income_block[0].text\n",
    "    return (team_name,expenditure,income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need a function to apply above function to each entry\n",
    "def get_all_tuples(entries):\n",
    "    tuples = []\n",
    "    for entry in entries:\n",
    "        data = get_tuple_data(entry)\n",
    "        tuples.append(data)\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need a function to, for a single page, get all the data on that page\n",
    "def get_page_data(url):\n",
    "    # get the page html\n",
    "    response = requests.get(url, headers={'User-Agent': 'Custom'})\n",
    "    page = soup(response.content, \"html.parser\")\n",
    "    \n",
    "    # get all table entries\n",
    "    odd_entries = page.findAll(\"tr\",{\"class\":\"odd\"})\n",
    "    even_entries = page.findAll(\"tr\",{\"class\":\"even\"})\n",
    "    entries = odd_entries + even_entries\n",
    "    \n",
    "    tuples = get_all_tuples(entries)\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to get all page data, for all urls\n",
    "def get_all_pages_data(url_dict):\n",
    "    df = pd.DataFrame(data=[],columns=['Year','Team','Expenditure','Income'])\n",
    "    for key in url_dict.keys():\n",
    "        year_col = [str(key)]*20\n",
    "        url = url_dict.get(key)\n",
    "        tuples = get_page_data(url)\n",
    "        data = [(year_col[i],tuples[i][0],tuples[i][1],tuples[i][2]) for i in range(20)]\n",
    "        df_new = pd.DataFrame(data=data,columns=['Year','Team','Expenditure','Income'])\n",
    "        df = pd.concat([df,df_new],axis=0)\n",
    "        print(\"Year {} Complete\".format(key))\n",
    "        time.sleep(60)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 1995 Complete\n",
      "Year 1996 Complete\n",
      "Year 1997 Complete\n",
      "Year 1998 Complete\n",
      "Year 1999 Complete\n",
      "Year 2000 Complete\n",
      "Year 2001 Complete\n",
      "Year 2002 Complete\n",
      "Year 2003 Complete\n",
      "Year 2004 Complete\n",
      "Year 2005 Complete\n",
      "Year 2006 Complete\n",
      "Year 2007 Complete\n",
      "Year 2008 Complete\n",
      "Year 2009 Complete\n",
      "Year 2010 Complete\n",
      "Year 2011 Complete\n",
      "Year 2012 Complete\n",
      "Year 2013 Complete\n",
      "Year 2014 Complete\n",
      "Year 2015 Complete\n",
      "Year 2016 Complete\n",
      "Year 2017 Complete\n",
      "Year 2018 Complete\n"
     ]
    }
   ],
   "source": [
    "# now using what we have, we can write all this data to a csv\n",
    "csv_file = 'transfer_data.csv'\n",
    "df = get_all_pages_data(url_dict)\n",
    "df.to_csv(csv_file,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
